import argparse
import sys
import os
import json
import re
from typing import List, Dict, Set, Tuple
from time import time
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from difflib import SequenceMatcher
from pprint import pprint
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np


def calculate_precision_recall_f1(gold: Set, pred) -> (float, float, float):
    """
    Method to calculate precision, recall and f1:
        Precision is calculated as correct_triples/predicted_triples and
        Recall as correct_triples/gold_triples
        F1 as the harmonic mean of precision and recall.
    :param gold: items in the gold standard
    :param pred: items in the system prediction
    :return:
        p: float - precision
        r: float - recall
        f1: float - F1
    """
    if len(pred) == 0:
        return 0, 0, 0
    p = sum(pred == True) / len(pred)
    r = sum(pred == True) / len(gold)
    if p + r > 0:
        f1 = 2 * ((p * r) / (p + r))
    else:
        f1 = 0
    return p, r, f1


def get_subject_object_hallucinations(ps, c_lists, gt_triples, triples) -> (float, float):
    """
    Calculate subject and object hallucinations metrics. As the context for calculating hallucinations, we consider the
    test sentence and the ontology concepts as relevant tokens.
    :param ps: stemmer for stemming words before checking for hallucinations
    :param c_lists: list of concepts from ontology and ground truth
    :param gt_triples: ground truth triples
    :param triples: a set of triples generated by the system
    :return:
        subj_hallucination: float - subject hallucination metric
        obj_hallucination: float - object hallucination metric
    """
    # if the set of triples are empty, we return 0
    if len(triples) == 0:
        return 0, 0

    # Add ground truth entities to reference concepts
    for tr in gt_triples:
        if tr[0] not in c_lists:
            c_lists.append(tr[0])
        if tr[2] not in c_lists:
            c_lists.append(tr[2])

    # Extract unique subjects from the system
    subject_list = [tr[0] for tr in triples]
    s_list = list(dict.fromkeys(subject_list))
    subject_matching = similar(s_list, c_lists)

    # Extract unique objects from the system
    object_list = [tr[2] for tr in triples]
    o_list = list(dict.fromkeys(object_list))
    object_matching = similar(o_list, c_lists)

    # CORRECTION: Hallucination is the inverse of matching
    # If subject_matching[i] == True, then there is NO hallucination
    # If subject_matching[i] == False, then there is hallucination
    subj_hallucination = sum(subject_matching == False) / len(s_list)
    obj_hallucination = sum(object_matching == False) / len(o_list)
    
    return subj_hallucination, obj_hallucination


def get_ontology_conformance(ont_rels, triples: List) -> (float, float):
    """
    Calculate the ontology conformance and relation hallucination metrics.
    :param ont_rels: ontology relations
    :param triples: a set of triples generated by the system
    :return:
        ont_conformance: float - ontology conformance metric
        rel_hallucination: float - relation hallucination metric = 1 - ontology conformance
    """
    if len(triples) == 0:
        return 1, 0

    ont_rels.append("is_A")

    # count the number of system triples relations that are in the ontology
    num_rels_conformant = len([tr for tr in triples if tr[1] in ont_rels])

    # ontology conformance is the number of system triples relations in the ontology divided by the total number of system triples
    ont_conformance = num_rels_conformant / len(triples)
    # relation hallucination is 1 - ontology conformance
    rel_hallucination = 1 - ont_conformance
    return ont_conformance, rel_hallucination


def normalize_triple(sub_label: str, rel_label: str, obj_label: str) -> str:
    """
    Normalize triples for comparison in precision, recall calculations
    :param sub_label: subject string
    :param rel_label: relation string
    :param obj_label: object string
    :return: a normalized triple as a single concatenated string
    """
    # remove spaces and underscores and make lower case
    sub_label = re.sub(r"(_|\s+)", '', sub_label).lower()
    rel_label = re.sub(r"(_|\s+)", '', rel_label).lower()
    obj_label = re.sub(r"(_|\s+)", '', obj_label).lower()
    # concatenate them to a single string
    tr_key = f"{sub_label} {rel_label} {obj_label}"
    return tr_key


def clean_entity_string(ps, entity: str) -> str:
    """
    Utility method to clean subject and object strings of triples
    :param ps: stemmer for stemming words before checking for hallucinations
    :param entity: subject or object string
    :return: the cleaned and normalized string
    """
    # stem every word for better matches
    stemmed_entity = "".join([ps.stem(word) for word in word_tokenize(entity)])
    # normalizing the string by removing white spaces, underscores and then converting to lower case
    normalized_stemmed_entity = re.sub(r"(_|\s+)", '', stemmed_entity).lower()
    # special handling for string with years to remove January 01
    return normalized_stemmed_entity.replace("01januari", "")


def read_json(json_path: str) -> Dict:
    """
    Utility method for reading JSON files
    :param json_path: path to the json file
    :return: json file content as a dictionary
    """
    with open(json_path) as in_file:
        return json.load(in_file)


def load_json(src_file):
    with open(src_file, 'r', encoding='utf-8') as f:
        source = json.load(f)
        return source


def load_ontologies(src_files):
    data = [load_json("src/ontology_population_project/ontology/json/" + src_file + "_ontology.json") for src_file in src_files]
    return data



def convert_to_dict(data: List[Dict], id_name: str = "id") -> Dict:
    """
    Utility method to convert a list to a dictionary
    :param data: a list of dictionary objects
    :param id_name: the attribute to be used as the key for the dictionary
    :return: a dictionary with the same content as the list
    """
    return {item[id_name]: item for item in data}


def similar(system_triples, gt_triples, threshold=0.8):
    """
    Compare system triples with ground truth using semantic similarity
    :param system_triples: list of system generated triples/entities
    :param gt_triples: list of ground truth triples/entities
    :param threshold: similarity threshold
    :return: boolean array indicating matches
    """
    model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
    
    # If no system data, return empty array
    if len(system_triples) == 0:
        return np.array([])
    
    # If no ground truth data, everything is considered hallucination
    if len(gt_triples) == 0:
        return np.array([False] * len(system_triples))
    
    gt_embeddings = model.encode(gt_triples)
    system_embeddings = model.encode(system_triples)
    
    # Calculate similarity matrix
    # gt_embeddings: (n_gt, embedding_dim)
    # system_embeddings: (n_system, embedding_dim)
    # sim_matrix: (n_system, n_gt) - each row corresponds to a system element
    sim_matrix = cosine_similarity(system_embeddings, gt_embeddings)
    
    # For each system element, take the maximum similarity with any GT element
    y_pred = np.max(sim_matrix, axis=1)
    
    # Apply threshold
    y_pred = y_pred > threshold
    
    # Display unmatched elements (potential hallucinations)
    error_list = [s for s, y in zip(system_triples, y_pred) if y == False]
    if error_list:
        print("Unmatched elements:", error_list)
    
    return y_pred


def get_sub_rel_ob(ontology):
    concepts = []
    relations = []
    for o in ontology:
        relations += [rel['label'].replace(" ", "_") for rel in o['relations']]
        concepts += [c["label"] for c in o['concepts']]
    return concepts, relations

def save_json(data, jsonl_path: str) -> None:
    """
    Utility method to serialize a list of json objects to a .json file
    :param data: list of data items
    :param jsonl_path: path to the output .json file
    :return: None
    """
    with open(jsonl_path, "w", encoding='utf-8') as out_file:
        out_file.write(f"{json.dumps(data, indent=4, ensure_ascii=False)}\n")

def evaluation(system_output, ground_truth, case):
    ontology_list = ['Person', 'Activity', 'Time', 'Challenge', 'Profile', 'Environment', 'Situation']
    ontology = load_ontologies(ontology_list)

    # stemmer for stemming words before checking for hallucinations
    ps = PorterStemmer()

    # collect the ground truth triples
    gt_triples = [[tr['subject'], tr['predicate'], str(tr['object'])] for tr in ground_truth['triplets']]
    print("Ground truth triples: ", len(gt_triples))

    system_triples = [[tr['subject'], tr['predicate'], str(tr['object'])] for tr in system_output['triplets']]
    print("System triples: ", len(system_triples))

    # collect the set of relations in ground truth triples, spaces are converted to "_" to make them
    # comparable with system triples
    gt_relations = {tr[1].replace(" ", "_") for tr in gt_triples}

    # filter out any triples in system output that does not match with ground truth relations
    filtered_system_triples = [tr for tr in system_triples if tr[1] in gt_relations]
    print("Filtered system triples: ", len(filtered_system_triples))

    concepts, relations = get_sub_rel_ob(ontology)

    # create a normalized string from subject, relation, object of each triple for comparison
    normalized_system_triples = {normalize_triple(tr[0], tr[1], tr[2]) for tr in filtered_system_triples}
    normalized_gt_triples = {normalize_triple(tr[0], tr[1], tr[2]) for tr in gt_triples}

    n_system_triples = [normalize_triple(tr[0], tr[1], tr[2]) for tr in filtered_system_triples]
    n_gt_triples = [normalize_triple(tr[0], tr[1], tr[2]) for tr in gt_triples]

    y_pred = similar(n_system_triples, n_gt_triples)

    # compare the system output triples with ground truth triples and calculate precision, recall, f1
    precision, recall, f1 = calculate_precision_recall_f1(normalized_gt_triples, y_pred)

    # calculate ontology conformance and relation hallucination
    ont_conformance, rel_hallucination = get_ontology_conformance(relations.copy(), system_triples)

    # calculate subject and object hallucination
    subj_hallucination, obj_hallucination = get_subject_object_hallucinations(ps, concepts.copy(), gt_triples, system_triples)

    # Debug to understand cases where f1 < 1 but hallucinations = 0
    if f1 < 1 and len(filtered_system_triples) > 0 and subj_hallucination == 0 and obj_hallucination == 0:
        print(f"DEBUG - f1: {f1}")
        print(f"Filtered system triples: {len(filtered_system_triples)}")
        print(f"Ground truth triples: {len(gt_triples)}")
        print(f"Subject hallucination: {subj_hallucination}")
        print(f"Object hallucination: {obj_hallucination}")

    eval_metrics = {
        "precision": f"{precision:.2f}",
        "recall": f"{recall:.2f}",
        "f1": f"{f1:.2f}",
        "onto_conf": f"{ont_conformance:.2f}",
        "rel_halluc": f"{rel_hallucination:.2f}",
        "sub_halluc": f"{subj_hallucination:.2f}",
        "obj_halluc": f"{obj_hallucination:.2f}"
    }
    
    output = {
        "llm_triples": system_triples,
        "filtered_llm_triples": filtered_system_triples,
        "gt_triples": gt_triples
    }
    #save_json(eval_metrics, f"src/ontology_population_project/data/evaluation/{case}.json")

    pprint(eval_metrics)
    return eval_metrics, output